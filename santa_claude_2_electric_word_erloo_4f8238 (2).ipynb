{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04qAJcGQWGz",
   "metadata": {
    "id": "f04qAJcGQWGz"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mJwnow1jDxNo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "mJwnow1jDxNo",
    "outputId": "13ed8bd3-0f07-4f79-e07d-ce1ed52028f3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXf3N1uqQhej",
   "metadata": {
    "id": "xXf3N1uqQhej"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i-_mF-nuRjn8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-_mF-nuRjn8",
    "outputId": "3c2ad09f-8afd-47fc-f5f5-373d42ce7e91"
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "!kaggle models instances versions   download google/gemma-2/transformers/gemma-2-9b/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KdJ1I4B4SvWi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdJ1I4B4SvWi",
    "outputId": "9ac38061-a41b-4349-fdd6-612d5397e20a"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data/model\n",
    "!tar -xzvf gemma-2.tar.gz -C ./data/model\n",
    "!ls ./data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746d7df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-02T15:11:34.989563Z",
     "iopub.status.busy": "2025-01-02T15:11:34.989240Z",
     "iopub.status.idle": "2025-01-02T15:12:15.736685Z",
     "shell.execute_reply": "2025-01-02T15:12:15.735980Z"
    },
    "id": "8746d7df",
    "outputId": "6537ad68-4955-421d-a1d1-b25df9091c08",
    "papermill": {
     "duration": 40.765133,
     "end_time": "2025-01-02T15:12:15.738736",
     "exception": false,
     "start_time": "2025-01-02T15:11:34.973603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"notebook started..\")\n",
    "!pip install -q transformers==4.46.3\n",
    "# !pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "print(\"pip installs done!\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CLm-qRxtlorb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLm-qRxtlorb",
    "outputId": "67fdb990-504a-4fd3-f963-19596ae3419e"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(accelerate.__version__)\n",
    "print(bitsandbytes.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54c2ea",
   "metadata": {
    "id": "0d54c2ea",
    "papermill": {
     "duration": 0.013211,
     "end_time": "2025-01-02T15:12:15.766468",
     "exception": false,
     "start_time": "2025-01-02T15:12:15.753257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reload prior submission.csv and optimize from that!!!\n",
    "* Set `reuse_last_submission = False` to start fresh\n",
    "* Specify any samples we don't think need further optimization\n",
    "* Specify any samples we want to fully shuffle between optimization rounds (stuck at local optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c2eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T15:12:15.796235Z",
     "iopub.status.busy": "2025-01-02T15:12:15.795521Z",
     "iopub.status.idle": "2025-01-02T15:12:15.812434Z",
     "shell.execute_reply": "2025-01-02T15:12:15.811470Z"
    },
    "id": "161c2eec",
    "papermill": {
     "duration": 0.032987,
     "end_time": "2025-01-02T15:12:15.814253",
     "exception": false,
     "start_time": "2025-01-02T15:12:15.781266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to False to do a \"clean\" run\n",
    "reuse_last_submission = True\n",
    "\n",
    "if reuse_last_submission:\n",
    "    samples = pd.read_csv(\"/content/submission (57).csv\")\n",
    "else:\n",
    "    samples = pd.read_csv(\"/content/submission (57).csv\")\n",
    "\n",
    "# If we think some items are well-optimized - including them here will skip them\n",
    "skip = [0, 1, 2, 3, 5]\n",
    "\n",
    "# If some samples are badly stuck at local optimum - we fully shuffle between rounds\n",
    "shuffle_between_cycles = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92199a",
   "metadata": {
    "id": "fd92199a",
    "papermill": {
     "duration": 0.013948,
     "end_time": "2025-01-02T15:12:15.904598",
     "exception": false,
     "start_time": "2025-01-02T15:12:15.890650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Batch-enabled scorer\n",
    "* Re-enabled 8-bit quantization\n",
    "* Make use of T4x2 setup\n",
    "* Credit: https://www.kaggle.com/code/cdeotte/brute-force-first-sample-perplexity-470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70428a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T15:12:15.934394Z",
     "iopub.status.busy": "2025-01-02T15:12:15.934116Z",
     "iopub.status.idle": "2025-01-02T15:12:20.549229Z",
     "shell.execute_reply": "2025-01-02T15:12:20.548547Z"
    },
    "id": "7a70428a",
    "papermill": {
     "duration": 4.632284,
     "end_time": "2025-01-02T15:12:20.551327",
     "exception": false,
     "start_time": "2025-01-02T15:12:15.919043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = True,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the mean perplexity of submitted text permutations compared to an original text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : DataFrame\n",
    "        DataFrame containing the original text in a column named 'text'.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    submission : DataFrame\n",
    "        DataFrame containing the permuted text in a column named 'text'.\n",
    "        Must have the same row IDs as the solution.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    row_id_column_name : str\n",
    "        Name of the column containing row IDs.\n",
    "        Ensures aligned comparison between solution and submission.\n",
    "\n",
    "    model_path : str\n",
    "        Path to the serialized LLM.\n",
    "\n",
    "    clear_mem : bool\n",
    "        Clear GPU memory after scoring by clearing the CUDA cache.\n",
    "        Useful for testing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean perplexity score. Lower is better.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParticipantVisibleError\n",
    "        If the submission format is invalid or submitted strings are not valid permutations.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n",
    "    ... })\n",
    "    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n",
    "    True\n",
    "    \"\"\"\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    \"\"\"\n",
    "    Calculates perplexity of text using a pre-trained language model.\n",
    "\n",
    "    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to the pre-trained language model\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    device_map : str, default=\"auto\"\n",
    "        Device mapping for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path,padding_side=\"right\")\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "\n",
    "            #quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            #quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "            quantization_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit = True,\n",
    "                bnb_4bit_quant_type = \"fp4\", #fp4 nf4\n",
    "                bnb_4bit_use_double_quant = False,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "        #if not load_in_8bit:\n",
    "        #    self.model.to(DEVICE)  # Explicitly move the model to the device\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], batch_size: 32\n",
    "    ) -> Union[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of given texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_texts : str or list of str\n",
    "            A single string or a list of strings.\n",
    "\n",
    "        batch_size : int, default=None\n",
    "            Batch size for processing. Defaults to the number of input texts.\n",
    "\n",
    "        verbose : bool, default=False\n",
    "            Display progress bar.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or list of float\n",
    "            A single perplexity value if input is a single string,\n",
    "            or a list of perplexity values if input is a list of strings.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import pandas as pd\n",
    "        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "        >>> scorer = PerplexityCalculator(model_path=model_path)\n",
    "\n",
    "        >>> submission = pd.DataFrame({\n",
    "        ...     'id': [0, 1, 2],\n",
    "        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "        ... })\n",
    "        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "        >>> perplexities[0] < perplexities[1]\n",
    "        True\n",
    "        >>> perplexities[2] < perplexities[0]\n",
    "        True\n",
    "\n",
    "        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n",
    "        >>> all(p > 0 for p in perplexities)\n",
    "        True\n",
    "\n",
    "        >>> scorer.clear_gpu_memory()\n",
    "        \"\"\"\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "        batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n",
    "        for j in range(batches):\n",
    "\n",
    "            a = j*batch_size\n",
    "            b = (j+1)*batch_size\n",
    "            input_batch = input_texts[a:b]\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                    padding=True\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                label = model_inputs['input_ids']\n",
    "                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = label[..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                loss = loss.view(len(logits), -1)\n",
    "                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n",
    "                loss = torch.sum(loss, -1) / valid_length\n",
    "\n",
    "                loss_list += loss.cpu().tolist()\n",
    "\n",
    "                # Debug output\n",
    "                #print(f\"\\nProcessing: '{text}'\")\n",
    "                #print(f\"With special tokens: '{text_with_special}'\")\n",
    "                #print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                #print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                #print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                #print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                #print(f\"Individual losses: {loss.tolist()}\")\n",
    "                #print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        # print(\"\\nFinal perplexities:\")\n",
    "        # for text, perp in zip(input_texts, ppl):\n",
    "        #     print(f\"Text: '{text}'\")\n",
    "        #     print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8395e7",
   "metadata": {
    "id": "bc8395e7",
    "papermill": {
     "duration": 0.015009,
     "end_time": "2025-01-02T15:12:20.580820",
     "exception": false,
     "start_time": "2025-01-02T15:12:20.565811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load custom scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c30f38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "590000c9426543eb8b0a71d524ab2a76",
      "6d36152098f941438d99cfccb8ec78f0",
      "f92c8707785e441590370a175f2ad04d",
      "0ceb94b8ecfc48d78b8c8895310ad8bd",
      "1ebfa98bd64e4c8db6247348c8277eb9",
      "06590c3e459f4190ac2f39847fb106e9",
      "625aff2a10ba427f90aba82805ac23c3",
      "e52c7d67ec114949b92eb4c0ab74ce23",
      "222561f3f0ce4d1a8b5e4e3cfd493391",
      "a0f5c58291ce4379ae75fc083dff21bb",
      "219a9c7f5ce04a6994c4e8af8e22468b"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-01-02T15:12:20.610445Z",
     "iopub.status.busy": "2025-01-02T15:12:20.610050Z",
     "iopub.status.idle": "2025-01-02T15:15:10.708083Z",
     "shell.execute_reply": "2025-01-02T15:15:10.707091Z"
    },
    "id": "f5c30f38",
    "outputId": "8de2499b-5873-4e5d-b3f9-bcbe4ba8ad84",
    "papermill": {
     "duration": 170.114914,
     "end_time": "2025-01-02T15:15:10.710294",
     "exception": false,
     "start_time": "2025-01-02T15:12:20.595380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#gemma-2-9b (competition scoring metric)\n",
    "scorer = PerplexityCalculator('/content/data/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305484aa",
   "metadata": {
    "id": "305484aa",
    "papermill": {
     "duration": 0.014083,
     "end_time": "2025-01-02T15:15:10.739524",
     "exception": false,
     "start_time": "2025-01-02T15:15:10.725441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get our starting scores\n",
    "* On initial dataset scorer returns some NaN's - we'll account for that..\n",
    "* Since we've re-run this notebook a few times - further re-runs may not improve the score much..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b4407",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-02T15:15:10.770600Z",
     "iopub.status.busy": "2025-01-02T15:15:10.770027Z",
     "iopub.status.idle": "2025-01-02T15:15:12.494334Z",
     "shell.execute_reply": "2025-01-02T15:15:12.492994Z"
    },
    "id": "b73b4407",
    "outputId": "b788e766-d23f-46e9-f1aa-f78bd14a9635",
    "papermill": {
     "duration": 1.741814,
     "end_time": "2025-01-02T15:15:12.496220",
     "exception": false,
     "start_time": "2025-01-02T15:15:10.754406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get actual mean value\n",
    "scores = []\n",
    "for row in range(len(samples)):\n",
    "    score = scorer.get_perplexity(samples.iloc[row].text, batch_size=1)\n",
    "    print(samples.iloc[row].text)\n",
    "    print(f\"Score: {score:.2f}\\n\")\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Starting mean score: {np.mean(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S0jxSu0YeuwU",
   "metadata": {
    "id": "S0jxSu0YeuwU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tdoj7RGz0qwf",
   "metadata": {
    "id": "Tdoj7RGz0qwf"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "def move_random_word_to_front(seq: List[str]) -> List[str]:\n",
    "    \"\"\"Pick a random word from seq and move it to the front.\"\"\"\n",
    "    if not seq:\n",
    "        return seq\n",
    "    neighbor = seq.copy()\n",
    "    idx = random.randrange(len(neighbor))\n",
    "    word = neighbor.pop(idx)\n",
    "    neighbor.insert(0, word)\n",
    "    return neighbor\n",
    "\n",
    "def generate_neighbors(current: List[str], strategy: str = \"swap\", k: int = 8) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate a neighbor of the current sequence based on the specified strategy.\n",
    "\n",
    "    Args:\n",
    "        current (List[str]): The current sequence of words.\n",
    "        strategy (str): The strategy for generating a neighbor. Options are:\n",
    "            - \"swap\"\n",
    "            - \"reverse\"\n",
    "            - \"shuffle_subsequence\"\n",
    "            - \"shift\"\n",
    "            - \"partial_permutation\"\n",
    "            - \"shift_then_partial_permutation\"\n",
    "            - \"partial_permutation_k3\"\n",
    "            - \"shift_then_partial_permutation_k3\"\n",
    "        k (int): Number of words to permute for partial_permutation-based strategies.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A new sequence generated by the specified strategy.\n",
    "    \"\"\"\n",
    "    neighbor = current.copy()\n",
    "    length = len(neighbor)\n",
    "\n",
    "    if length < 2:\n",
    "        return neighbor  # No valid mutation possible\n",
    "\n",
    "    if strategy == \"swap\":\n",
    "        i, j = random.sample(range(length), 2)\n",
    "        neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
    "\n",
    "    elif strategy == \"reverse\":\n",
    "        i, j = sorted(random.sample(range(length), 2))\n",
    "        neighbor[i:j+1] = reversed(neighbor[i:j+1])\n",
    "\n",
    "    elif strategy == \"shuffle_subsequence\":\n",
    "        i, j = sorted(random.sample(range(length), 2))\n",
    "        subsequence = neighbor[i:j+1]\n",
    "        random.shuffle(subsequence)\n",
    "        neighbor[i:j+1] = subsequence\n",
    "\n",
    "    elif strategy == \"shift\":\n",
    "        i, j = sorted(random.sample(range(length), 2))\n",
    "        subsequence = neighbor[i:j+1]\n",
    "        if random.choice([True, False]):  # Rotate left\n",
    "            subsequence = subsequence[1:] + subsequence[:1]\n",
    "        else:  # Rotate right\n",
    "            subsequence = subsequence[-1:] + subsequence[:-1]\n",
    "        neighbor[i:j+1] = subsequence\n",
    "\n",
    "    elif strategy == \"partial_permutation\":\n",
    "        indices = random.sample(range(length), min(k, length))\n",
    "        permuted_values = random.sample([neighbor[i] for i in indices], len(indices))\n",
    "        for idx, new_val in zip(indices, permuted_values):\n",
    "            neighbor[idx] = new_val\n",
    "\n",
    "    elif strategy == \"shift_then_partial_permutation\":\n",
    "        # Step 1: Shift a random subsequence\n",
    "        i, j = sorted(random.sample(range(length), 2))\n",
    "        subsequence = neighbor[i:j+1]\n",
    "        if random.choice([True, False]):  # Rotate left\n",
    "            subsequence = subsequence[1:] + subsequence[:1]\n",
    "        else:  # Rotate right\n",
    "            subsequence = subsequence[-1:] + subsequence[:-1]\n",
    "        neighbor[i:j+1] = subsequence\n",
    "\n",
    "        # Step 2: Apply partial permutation\n",
    "        indices = random.sample(range(length), min(k, length))\n",
    "        permuted_values = random.sample([neighbor[i] for i in indices], len(indices))\n",
    "        for idx, new_val in zip(indices, permuted_values):\n",
    "            neighbor[idx] = new_val\n",
    "\n",
    "    elif strategy == \"partial_permutation_k3\":\n",
    "        k_fixed = 3\n",
    "        indices = random.sample(range(length), min(k_fixed, length))\n",
    "        permuted_values = random.sample([neighbor[i] for i in indices], len(indices))\n",
    "        for idx, new_val in zip(indices, permuted_values):\n",
    "            neighbor[idx] = new_val\n",
    "\n",
    "    elif strategy == \"shift_then_partial_permutation_k3\":\n",
    "        # Step 1: Shift a random subsequence\n",
    "        i, j = sorted(random.sample(range(length), 2))\n",
    "        subsequence = neighbor[i:j+1]\n",
    "        if random.choice([True, False]):\n",
    "            subsequence = subsequence[1:] + subsequence[:1]\n",
    "        else:\n",
    "            subsequence = subsequence[-1:] + subsequence[:-1]\n",
    "        neighbor[i:j+1] = subsequence\n",
    "\n",
    "        # Step 2: Partial permutation with k=3\n",
    "        k_fixed = 3\n",
    "        indices = random.sample(range(length), min(k_fixed, length))\n",
    "        permuted_values = random.sample([neighbor[i] for i in indices], len(indices))\n",
    "        for idx, new_val in zip(indices, permuted_values):\n",
    "            neighbor[idx] = new_val\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    return neighbor\n",
    "\n",
    "\n",
    "def simulated_annealing_optimize(\n",
    "        text: str,\n",
    "        scorer,\n",
    "        temp_start: float = 3.0,\n",
    "        temp_end: float = 0.02,\n",
    "        cooling_rate: float = 0.99,\n",
    "        samples_per_temp: int = 250,\n",
    "        max_batch_size: int = 64,\n",
    "        max_words_per_batch: int = 640,\n",
    "        reheat_cycles: int = 8,\n",
    "        low_temp_samples_after_improve: int = 1000,\n",
    "        temp_start_increase_per_cycle: float = 0.5,\n",
    "        shuffle_chance_between_cycles: float = 1.0,\n",
    "        verbose: bool = False,\n",
    "        initial_sequence: List[str] = None,\n",
    "        perplexity_threshold: float = 80.0,\n",
    "        terminate_on_threshold: bool = False\n",
    "    ) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Optimize the ordering of words in text to minimize perplexity via simulated annealing.\n",
    "\n",
    "    In this version, we apply a 'move_random_word_to_front' perturbation\n",
    "    before each reheat cycle, then start from that perturbed sequence.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    if initial_sequence is not None:\n",
    "        current = initial_sequence.copy()\n",
    "    else:\n",
    "        current = words.copy()\n",
    "\n",
    "    # Score the initial sequence\n",
    "    current_text = ' '.join(current)\n",
    "    current_score = scorer.get_perplexity(current_text, batch_size=1)\n",
    "    attempts = 0\n",
    "    while math.isnan(current_score) and attempts < 10:\n",
    "        # If the score is NaN, shuffle to see if we get a valid perplexity\n",
    "        random.shuffle(current)\n",
    "        current_text = ' '.join(current)\n",
    "        current_score = scorer.get_perplexity(current_text, batch_size=1)\n",
    "        attempts += 1\n",
    "\n",
    "    if math.isnan(current_score):\n",
    "        raise ValueError(\"Unable to find a valid initial permutation (NaN perplexity).\")\n",
    "\n",
    "    # Keep track of the overall best within this run\n",
    "    best = current.copy()\n",
    "    best_score = current_score\n",
    "\n",
    "    print(f\"Initial score: {current_score:.2f}\")\n",
    "\n",
    "    # Weighted strategies (giving higher probability to partial_permutation_k3, etc.)\n",
    "    weighted_strategies = [\n",
    "        (\"swap\", 1),\n",
    "        (\"shift\", 1),\n",
    "        (\"reverse\", 1),\n",
    "        (\"shuffle_subsequence\", 1),\n",
    "        (\"partial_permutation\", 1),\n",
    "        (\"shift_then_partial_permutation\", 1),\n",
    "        (\"partial_permutation_k3\", 3),\n",
    "        (\"shift_then_partial_permutation_k3\", 3),\n",
    "    ]\n",
    "\n",
    "    # Prepare batch/neighbor logic\n",
    "    max_neighbors = max_words_per_batch // len(words) if len(words) != 0 else 1\n",
    "    if max_neighbors == 0:\n",
    "        max_neighbors = 1\n",
    "    actual_batch_size = min(max_batch_size, max_neighbors)\n",
    "    if actual_batch_size == 0:\n",
    "        actual_batch_size = 1\n",
    "    batches_per_temp = (samples_per_temp + actual_batch_size - 1) // actual_batch_size\n",
    "\n",
    "    # Tracking stats\n",
    "    total_moves = 0\n",
    "    accepted_moves = 0\n",
    "    improved_moves = 0\n",
    "    rejected_moves = 0\n",
    "\n",
    "    def print_stats():\n",
    "        if total_moves == 0:\n",
    "            return\n",
    "        acceptance_rate = accepted_moves / total_moves\n",
    "        improvement_rate = improved_moves / total_moves\n",
    "        print(\n",
    "            f\"  [Stats] Moves={total_moves}, Accepted={accepted_moves}, Rejected={rejected_moves}, \"\n",
    "            f\"AcceptanceRate={acceptance_rate:.3f}, ImprovementRate={improvement_rate:.3f}, \"\n",
    "            f\"CurrentScore={current_score:.2f}, BestScore={best_score:.2f}\"\n",
    "        )\n",
    "\n",
    "    reject_counter = 0\n",
    "    initial_temp_value = temp_start\n",
    "\n",
    "    # Main reheat cycles\n",
    "    for cycle in range(reheat_cycles):\n",
    "        # Before each cycle, apply the small perturbation to our current best\n",
    "        best = move_random_word_to_front(best)\n",
    "        current = best.copy()  # Start from that perturbed solution\n",
    "        current_score = scorer.get_perplexity(' '.join(current), batch_size=1)\n",
    "\n",
    "        temp = temp_start\n",
    "        scaling_factor = 1.0\n",
    "\n",
    "        print(f\"\\n=== Cycle {cycle + 1}/{reheat_cycles} - Starting temp: {temp:.2f} ===\")\n",
    "        print(f\"Perturbed score: {current_score:.2f}\")\n",
    "\n",
    "        remaining_low_temp_steps = 0\n",
    "        stored_temp = None\n",
    "\n",
    "        # Loop while temp > temp_end OR in low-temp mode\n",
    "        while temp > temp_end or remaining_low_temp_steps > 0:\n",
    "            samples_processed = 0\n",
    "\n",
    "            for _ in range(batches_per_temp):\n",
    "                remaining_samples = samples_per_temp - samples_processed\n",
    "                current_batch_size = min(actual_batch_size, remaining_samples)\n",
    "                if current_batch_size <= 0:\n",
    "                    break\n",
    "\n",
    "                neighbors = []\n",
    "                neighbor_texts = []\n",
    "\n",
    "                for _ in range(current_batch_size):\n",
    "                    population = [s for s, w in weighted_strategies]\n",
    "                    weights = [w for s, w in weighted_strategies]\n",
    "                    chosen_strategy = random.choices(population, weights=weights, k=1)[0]\n",
    "\n",
    "                    neighbor = generate_neighbors(current, strategy=chosen_strategy, k=8)\n",
    "                    neighbors.append(neighbor)\n",
    "                    neighbor_texts.append(' '.join(neighbor))\n",
    "\n",
    "                neighbor_scores = scorer.get_perplexity(neighbor_texts, batch_size=current_batch_size)\n",
    "\n",
    "                for neighbor, neighbor_score in zip(neighbors, neighbor_scores):\n",
    "                    samples_processed += 1\n",
    "                    total_moves += 1\n",
    "                    if math.isnan(neighbor_score):\n",
    "                        continue  # Skip NaNs\n",
    "\n",
    "                    delta = neighbor_score - current_score\n",
    "                    current_temp = temp_end if remaining_low_temp_steps > 0 else temp\n",
    "                    scaling_factor = max(1.0, temp / initial_temp_value)\n",
    "\n",
    "                    # Metropolis acceptance criterion\n",
    "                    if delta < 0:\n",
    "                        acceptance_probability = 1.0\n",
    "                    else:\n",
    "                        acceptance_probability = math.exp(-delta / (current_temp * scaling_factor))\n",
    "\n",
    "                    if random.random() < acceptance_probability:\n",
    "                        # ACCEPT\n",
    "                        accepted_moves += 1\n",
    "                        current = neighbor\n",
    "                        current_score = neighbor_score\n",
    "\n",
    "                        # Check if it's an improvement over the best\n",
    "                        if current_score < best_score:\n",
    "                            best = current.copy()\n",
    "                            best_score = current_score\n",
    "                            improved_moves += 1\n",
    "\n",
    "                            # If not already in low-temp mode, store temperature\n",
    "                            if remaining_low_temp_steps == 0:\n",
    "                                stored_temp = temp\n",
    "                            remaining_low_temp_steps = low_temp_samples_after_improve\n",
    "                            print(\">\", end=\"\", flush=True)\n",
    "                            if best_score < perplexity_threshold:\n",
    "                                print(f\"\\n*** Perplexity threshold reached: {best_score:.2f} ***\")\n",
    "                                print(f\"Best sequence: {' '.join(best)}\")\n",
    "                                if terminate_on_threshold:\n",
    "                                    print(\"Terminating optimization early as threshold was met.\")\n",
    "                                    return ' '.join(best), best_score\n",
    "                        else:\n",
    "                            print(\"<\", end=\"\", flush=True)\n",
    "                    else:\n",
    "                        # REJECT\n",
    "                        reject_counter += 1\n",
    "                        rejected_moves += 1\n",
    "                        if reject_counter % 10 == 0:\n",
    "                            print(\"-\", end=\"\", flush=True)\n",
    "\n",
    "            # Print intermediate stats\n",
    "            print_stats()\n",
    "\n",
    "            # Temperature / low-temp mode management\n",
    "            if remaining_low_temp_steps > 0:\n",
    "                remaining_low_temp_steps -= samples_per_temp\n",
    "                if verbose:\n",
    "                    print(f\"\\nLow-temp mode ({remaining_low_temp_steps} steps remaining). \"\n",
    "                          f\"Current={current_score:.2f}, Best={best_score:.2f}\")\n",
    "                if remaining_low_temp_steps <= 0 and stored_temp is not None:\n",
    "                    # Return to the stored temperature after low-temp mode\n",
    "                    temp = stored_temp\n",
    "            else:\n",
    "                # Standard cooling\n",
    "                temp *= cooling_rate\n",
    "                if verbose:\n",
    "                    print(f\"\\nCooling down: Temp={temp:.4f}, Current={current_score:.2f}, Best={best_score:.2f}\")\n",
    "\n",
    "        print(f\"\\nEnd of cycle {cycle + 1}, best score so far: {best_score:.2f}\")\n",
    "\n",
    "        # Optional shuffle between cycles\n",
    "        if random.random() < shuffle_chance_between_cycles:\n",
    "            random.shuffle(current)\n",
    "            current_score = scorer.get_perplexity(' '.join(current), batch_size=1)\n",
    "            print(f\"New shuffled score (between cycles): {current_score:.2f}\")\n",
    "\n",
    "        # Increase starting temp for the next cycle\n",
    "        temp_start += temp_start_increase_per_cycle\n",
    "\n",
    "    # Final output\n",
    "    print(\"\\n=== Optimization finished ===\")\n",
    "    print_stats()\n",
    "    print(f\"Final best score: {best_score:.2f}\")\n",
    "\n",
    "    return ' '.join(best), best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23506b76",
   "metadata": {
    "id": "23506b76",
    "papermill": {
     "duration": 0.01338,
     "end_time": "2025-01-02T15:15:12.603667",
     "exception": false,
     "start_time": "2025-01-02T15:15:12.590287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run and Submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3mrt5EbwLfMH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mrt5EbwLfMH",
    "outputId": "109e0628-022d-498a-eae7-014d4a166488"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BNCvKawoRYbR",
   "metadata": {
    "id": "BNCvKawoRYbR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9CkFfoA9RYX_",
   "metadata": {
    "id": "9CkFfoA9RYX_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eabc38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5eabc38",
    "outputId": "23f8fe9a-5e20-40b8-f8bb-c8c56b2838e8",
    "papermill": {
     "duration": 0.546916,
     "end_time": "2025-01-02T17:20:01.837267",
     "exception": false,
     "start_time": "2025-01-02T17:20:01.290351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text = \"from and of to the as in that it we with not you have milk chocolate candy fruitcake eggnog peppermint season greeting card wrapping paper bow toy doll game night puzzle cookie snowglobe star angel wreath poinsettia candle fireplace wish dream believe wonder hope joy peace merry hohoho kaggle workshop\"\n",
    "\n",
    "text = samples.loc[3, \"text\"]\n",
    "\n",
    "# text = \"from and of to the as in that it we with not you have milk chocolate candy fruitcake eggnog peppermint season greeting card wrapping paper bow toy doll game puzzle cookie snowglobe fireplace candle wreath poinsettia angel star night wish wonder dream believe hope joy peace merry hohoho kaggle workshop\"\n",
    "\n",
    "best_sequence, best_score = simulated_annealing_optimize(\n",
    "    text=text,\n",
    "    scorer=scorer,\n",
    "    temp_start=0.01,\n",
    "    temp_end=0.001,\n",
    "    cooling_rate=0.99,\n",
    "    samples_per_temp=250,\n",
    "    max_batch_size=64,\n",
    "    max_words_per_batch=640,\n",
    "    reheat_cycles=8,\n",
    "    low_temp_samples_after_improve=2000,\n",
    "    temp_start_increase_per_cycle=0.005,\n",
    "    shuffle_chance_between_cycles=0.0,\n",
    "    verbose=True,              # For detailed logs\n",
    "    perplexity_threshold=202.07,\n",
    "    terminate_on_threshold=False\n",
    ")\n",
    "\n",
    "print(f\"\\nBest optimized sequence: {best_sequence}\")\n",
    "print(f\"Best perplexity score: {best_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xinCi1uFtRsn",
   "metadata": {
    "id": "xinCi1uFtRsn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "datasetId": 6254992,
     "sourceId": 10135138,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256328,
     "sourceId": 10137065,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6288232,
     "sourceId": 10180024,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6345598,
     "sourceId": 10257911,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6345879,
     "sourceId": 10258345,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6389721,
     "sourceId": 10320463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10349303,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6410091,
     "sourceId": 10351674,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6410279,
     "sourceId": 10351946,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 213985603,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7713.39415,
   "end_time": "2025-01-02T17:20:05.456630",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-02T15:11:32.062480",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06590c3e459f4190ac2f39847fb106e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ceb94b8ecfc48d78b8c8895310ad8bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0f5c58291ce4379ae75fc083dff21bb",
      "placeholder": "​",
      "style": "IPY_MODEL_219a9c7f5ce04a6994c4e8af8e22468b",
      "value": " 8/8 [02:38&lt;00:00, 17.59s/it]"
     }
    },
    "1ebfa98bd64e4c8db6247348c8277eb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "219a9c7f5ce04a6994c4e8af8e22468b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "222561f3f0ce4d1a8b5e4e3cfd493391": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "590000c9426543eb8b0a71d524ab2a76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d36152098f941438d99cfccb8ec78f0",
       "IPY_MODEL_f92c8707785e441590370a175f2ad04d",
       "IPY_MODEL_0ceb94b8ecfc48d78b8c8895310ad8bd"
      ],
      "layout": "IPY_MODEL_1ebfa98bd64e4c8db6247348c8277eb9"
     }
    },
    "625aff2a10ba427f90aba82805ac23c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d36152098f941438d99cfccb8ec78f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06590c3e459f4190ac2f39847fb106e9",
      "placeholder": "​",
      "style": "IPY_MODEL_625aff2a10ba427f90aba82805ac23c3",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "a0f5c58291ce4379ae75fc083dff21bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e52c7d67ec114949b92eb4c0ab74ce23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f92c8707785e441590370a175f2ad04d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e52c7d67ec114949b92eb4c0ab74ce23",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_222561f3f0ce4d1a8b5e4e3cfd493391",
      "value": 8
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
